{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12c82f8b",
   "metadata": {},
   "source": [
    "# Chap 4: Generative models: Bayes Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709de13a",
   "metadata": {},
   "source": [
    "## General theory \n",
    "\n",
    "Logistic regression involves directly modeling $P(Y = k|X = x)$ using the logistic function. We now consider an alternative and less direct approach to estimating these probabilities. In this new approach, we model the distribution of the predictors $X$ separately in each of the response classes (i.e. for each value of $Y $). We then use Bayes’ theorem to flip these around into estimates for $P(Y = k|X = x)$. When the distribution of $X$ within each class is assumed to be normal, it turns out that the model is very similar in form to logistic regression.\n",
    "\n",
    "Recall that the generic form of Bayes' theorem is:\n",
    "\n",
    "- $\\text{Posterior} = \\text{Likelihood}\\times \\text{Prior}: \\,\\,P(Y|X) = \\dfrac{P(X|Y) \\times P(Y)}{P(X)}$\n",
    "\n",
    "\n",
    "\n",
    "Suppose that we wish to classify an observation into one of $K$ classes, where $K \\geq 2$. In other words, the qualitative response variable $Y$ can take on $K$ possible distinct and unordered values. \n",
    "- Let $\\pi_k$ represent the overall or prior probability that a randomly chosen observation comes from the $k$th class. \n",
    "\n",
    "- Let $f_k(X) \\equiv Pr(X|Y = k)$ denote the density function of $X$ for an observation that comes from the $k$th class. In other words, $f_k(x)$ is relatively large if there is a high probability that an observation in the $k$th class has $X \\approx x$, and $f_k(x)$ is small if it is very unlikely that an observation in the $k$th class has $X ≈ x$. \n",
    "\n",
    "- Then Bayes’ theorem states that\n",
    "    - $p_k(x) \\equiv P(Y = k|X=x) = \\dfrac{\\pi_k f_k(x)}{\\sum_{l=1}^K \\pi_l f_l(x)} = \\dfrac{\\pi_k P(X=x|Y = k)}{\\sum_{l=1}^K \\pi_l P(X=x|Y = l)}$\n",
    "    \n",
    "    \n",
    "The above equation means that instead of directly computing the posterior probability $p_k(x)$, we can simply plug in estimates of the prior $\\pi_k$ and likelihood $f_k(x)$ to get $p_k(x)$. In general, estimating $\\pi_k$ is easy if we have a random sample from the population: we simply compute the fraction of the training observations that belong to the $k$th class. However, estimating the density function $f_k(x)$ is much more challenging. As we will see, to estimate $f_k(x)$, we will typically have to make some simplifying assumptions. In the following sections, we discuss three classifiers that use different estimates of $f_k(x)$ to approximate the Bayes classifier: *linear discriminant analysis, quadratic discriminant analysis*, and *naive Bayes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75ccfa7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ISLR2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673e8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for MacOS 10.14 Mojave, install ISLR because the version of R is lower\n",
    "#library(ISLR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df42bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "attach(Smarket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e7f0ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 9</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Year</th><th scope=col>Lag1</th><th scope=col>Lag2</th><th scope=col>Lag3</th><th scope=col>Lag4</th><th scope=col>Lag5</th><th scope=col>Volume</th><th scope=col>Today</th><th scope=col>Direction</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;dbl&gt;</th><th scope=col>&lt;fct&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>2001</td><td> 0.381</td><td>-0.192</td><td>-2.624</td><td>-1.055</td><td> 5.010</td><td>1.1913</td><td> 0.959</td><td>Up  </td></tr>\n",
       "\t<tr><th scope=row>2</th><td>2001</td><td> 0.959</td><td> 0.381</td><td>-0.192</td><td>-2.624</td><td>-1.055</td><td>1.2965</td><td> 1.032</td><td>Up  </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>2001</td><td> 1.032</td><td> 0.959</td><td> 0.381</td><td>-0.192</td><td>-2.624</td><td>1.4112</td><td>-0.623</td><td>Down</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>2001</td><td>-0.623</td><td> 1.032</td><td> 0.959</td><td> 0.381</td><td>-0.192</td><td>1.2760</td><td> 0.614</td><td>Up  </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>2001</td><td> 0.614</td><td>-0.623</td><td> 1.032</td><td> 0.959</td><td> 0.381</td><td>1.2057</td><td> 0.213</td><td>Up  </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>2001</td><td> 0.213</td><td> 0.614</td><td>-0.623</td><td> 1.032</td><td> 0.959</td><td>1.3491</td><td> 1.392</td><td>Up  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 9\n",
       "\\begin{tabular}{r|lllllllll}\n",
       "  & Year & Lag1 & Lag2 & Lag3 & Lag4 & Lag5 & Volume & Today & Direction\\\\\n",
       "  & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <dbl> & <fct>\\\\\n",
       "\\hline\n",
       "\t1 & 2001 &  0.381 & -0.192 & -2.624 & -1.055 &  5.010 & 1.1913 &  0.959 & Up  \\\\\n",
       "\t2 & 2001 &  0.959 &  0.381 & -0.192 & -2.624 & -1.055 & 1.2965 &  1.032 & Up  \\\\\n",
       "\t3 & 2001 &  1.032 &  0.959 &  0.381 & -0.192 & -2.624 & 1.4112 & -0.623 & Down\\\\\n",
       "\t4 & 2001 & -0.623 &  1.032 &  0.959 &  0.381 & -0.192 & 1.2760 &  0.614 & Up  \\\\\n",
       "\t5 & 2001 &  0.614 & -0.623 &  1.032 &  0.959 &  0.381 & 1.2057 &  0.213 & Up  \\\\\n",
       "\t6 & 2001 &  0.213 &  0.614 & -0.623 &  1.032 &  0.959 & 1.3491 &  1.392 & Up  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 9\n",
       "\n",
       "| <!--/--> | Year &lt;dbl&gt; | Lag1 &lt;dbl&gt; | Lag2 &lt;dbl&gt; | Lag3 &lt;dbl&gt; | Lag4 &lt;dbl&gt; | Lag5 &lt;dbl&gt; | Volume &lt;dbl&gt; | Today &lt;dbl&gt; | Direction &lt;fct&gt; |\n",
       "|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1 | 2001 |  0.381 | -0.192 | -2.624 | -1.055 |  5.010 | 1.1913 |  0.959 | Up   |\n",
       "| 2 | 2001 |  0.959 |  0.381 | -0.192 | -2.624 | -1.055 | 1.2965 |  1.032 | Up   |\n",
       "| 3 | 2001 |  1.032 |  0.959 |  0.381 | -0.192 | -2.624 | 1.4112 | -0.623 | Down |\n",
       "| 4 | 2001 | -0.623 |  1.032 |  0.959 |  0.381 | -0.192 | 1.2760 |  0.614 | Up   |\n",
       "| 5 | 2001 |  0.614 | -0.623 |  1.032 |  0.959 |  0.381 | 1.2057 |  0.213 | Up   |\n",
       "| 6 | 2001 |  0.213 |  0.614 | -0.623 |  1.032 |  0.959 | 1.3491 |  1.392 | Up   |\n",
       "\n"
      ],
      "text/plain": [
       "  Year Lag1   Lag2   Lag3   Lag4   Lag5   Volume Today  Direction\n",
       "1 2001  0.381 -0.192 -2.624 -1.055  5.010 1.1913  0.959 Up       \n",
       "2 2001  0.959  0.381 -0.192 -2.624 -1.055 1.2965  1.032 Up       \n",
       "3 2001  1.032  0.959  0.381 -0.192 -2.624 1.4112 -0.623 Down     \n",
       "4 2001 -0.623  1.032  0.959  0.381 -0.192 1.2760  0.614 Up       \n",
       "5 2001  0.614 -0.623  1.032  0.959  0.381 1.2057  0.213 Up       \n",
       "6 2001  0.213  0.614 -0.623  1.032  0.959 1.3491  1.392 Up       "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "head(Smarket)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518fb7b0",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ffaba8",
   "metadata": {},
   "source": [
    "## LDA Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9476b4ad",
   "metadata": {},
   "source": [
    "**One predictor**\n",
    "\n",
    "In LDA, we assume that $f_k(x)$ is normal or Gaussian. Let us focus on the case $p=1$ (there is only 1 predictor), such that the Gaussian distribution is\n",
    "\n",
    "$f_k(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp\\left(-\\dfrac{1}{2\\sigma_k^2}(x-\\mu_k)^2\\right)$\n",
    "\n",
    "where $\\mu_k$ and $\\sigma_k^2$ are the mean and variance parameters for the $k$th class. For now, let us further assume that $\\sigma_1^2 = \\ldots = \\sigma_K^2$ : that is, there is a shared variance term across all $K$ classes, which for simplicity we can denote by $\\sigma^2$. Substituting this form of $f_k(x)$ into the Bayes' equation gives\n",
    "\n",
    "$p_k(x) = \\dfrac{ \\dfrac{\\pi_k}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\dfrac{1}{2\\sigma^2}(x-\\mu_k)^2\\right)}{\\sum_{l=1}^K  \\dfrac{\\pi_l}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\dfrac{1}{2\\sigma^2}(x-\\mu_l)^2\\right)}$\n",
    "\n",
    "The Bayes classifier involves assigning an observation $X = x$ to the class for which $p_k(x)$ is largest. This is  equivalent to assigning the observation to the class for which the following quantity is the largest:\n",
    "\n",
    "$\\delta_k(x) = x\\,.\\,\\dfrac{\\mu_k}{\\sigma^2} - \\dfrac{\\mu_k^2}{2\\sigma^2} + \\log(\\pi_k)$\n",
    "\n",
    "The linear discriminant analysis (LDA) method approximates the Bayes classifier by plugging estimates for $\\pi_k$, $\\mu_k$, and $\\sigma^2$ into the above equation. \n",
    "\n",
    "**Multiple predictors**\n",
    "\n",
    "We now extend the LDA classifier to the case of $p$ multiple predictors. To do this, we will assume that $X = (X_1,X_2, \\ldots, X_p)$ is drawn from a multivariate Gaussian (or multivariate normal) $N(\\mu_k, \\Sigma)$ distribution, with a class-specific $p$-dimensional mean vector $\\mu_k$ and a common covariance $p\\times p$ matrix $\\Sigma$. \n",
    "\n",
    "$f_k(x) = \\dfrac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}}\\exp\\left(-\\dfrac{1}{2}(x-\\mu_k)^T \\Sigma^{-1}(x-\\mu_k)\\right)$\n",
    "\n",
    "For this case, the Bayes classifier assigns an observation $X = x$ to the class for which\n",
    "\n",
    "$\\delta_k(x) = x^T \\Sigma^{-1} \\mu_k - \\dfrac{1}{2}\\mu^T_k \\Sigma^{-1}\\mu_k + \\log\\pi_k$\n",
    "\n",
    "is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c7d5d6",
   "metadata": {},
   "source": [
    "## LDA applied to the dataset 'Smarket'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a115bfe",
   "metadata": {},
   "source": [
    "In R, we fit an LDA model using the lda() function, which is part of the MASS library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eed1c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘MASS’\n",
      "\n",
      "\n",
      "The following object is masked from ‘package:ISLR2’:\n",
      "\n",
      "    Boston\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(MASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf5425d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a train set\n",
    "train <- (Year < 2005)\n",
    "Smarket_2005 <- Smarket[!train, ]\n",
    "Direction_2005 <- Direction[!train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67bdb86b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544\n",
       "\n",
       "Coefficients of linear discriminants:\n",
       "            LD1\n",
       "Lag1 -0.6420190\n",
       "Lag2 -0.5135293"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda.fit <- lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
    "lda.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023d5d4",
   "metadata": {},
   "source": [
    "The LDA output indicates that the priors are $\\hat{\\pi}_1 = 0.492$ and $\\hat{\\pi}_2 = 0.508$; in other words, 49.2% of the training observations correspond to days during which the market went down. It also provides the group means; these are the average of each predictor within each class, and are used by LDA as estimates of $\\mu_k$.\n",
    "\n",
    "The coefficients of linear discriminants output provides the linear combination of *Lag1* and *Lag2* that are used to form the LDA decision rule. In other words, these are the multipliers of the elements of $X = x$. If $(-0.642 \\times Lag1 − 0.514 \\times Lag2)$ is large, then the LDA classifier will predict a market increase, and if it is small, then the LDA classifier will predict a market decline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e5312b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       ".list-inline {list-style: none; margin:0; padding: 0}\n",
       ".list-inline>li {display: inline-block}\n",
       ".list-inline>li:not(:last-child)::after {content: \"\\00b7\"; padding: 0 .5ex}\n",
       "</style>\n",
       "<ol class=list-inline><li>'class'</li><li>'posterior'</li><li>'x'</li></ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'class'\n",
       "\\item 'posterior'\n",
       "\\item 'x'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'class'\n",
       "2. 'posterior'\n",
       "3. 'x'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"class\"     \"posterior\" \"x\"        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prediction\n",
    "lda_pred <- predict(lda.fit, Smarket_2005)\n",
    "names(lda_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c424fa27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Direction_2005\n",
       "lda_class Down  Up\n",
       "     Down   35  35\n",
       "     Up     76 106"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lda_class <- lda_pred$class\n",
    "#confusion matrix\n",
    "table(lda_class, Direction_2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b64a2558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A matrix: 6 × 2 of type dbl</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>Down</th><th scope=col>Up</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>999</th><td>0.4901792</td><td>0.5098208</td></tr>\n",
       "\t<tr><th scope=row>1000</th><td>0.4792185</td><td>0.5207815</td></tr>\n",
       "\t<tr><th scope=row>1001</th><td>0.4668185</td><td>0.5331815</td></tr>\n",
       "\t<tr><th scope=row>1002</th><td>0.4740011</td><td>0.5259989</td></tr>\n",
       "\t<tr><th scope=row>1003</th><td>0.4927877</td><td>0.5072123</td></tr>\n",
       "\t<tr><th scope=row>1004</th><td>0.4938562</td><td>0.5061438</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A matrix: 6 × 2 of type dbl\n",
       "\\begin{tabular}{r|ll}\n",
       "  & Down & Up\\\\\n",
       "\\hline\n",
       "\t999 & 0.4901792 & 0.5098208\\\\\n",
       "\t1000 & 0.4792185 & 0.5207815\\\\\n",
       "\t1001 & 0.4668185 & 0.5331815\\\\\n",
       "\t1002 & 0.4740011 & 0.5259989\\\\\n",
       "\t1003 & 0.4927877 & 0.5072123\\\\\n",
       "\t1004 & 0.4938562 & 0.5061438\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A matrix: 6 × 2 of type dbl\n",
       "\n",
       "| <!--/--> | Down | Up |\n",
       "|---|---|---|\n",
       "| 999 | 0.4901792 | 0.5098208 |\n",
       "| 1000 | 0.4792185 | 0.5207815 |\n",
       "| 1001 | 0.4668185 | 0.5331815 |\n",
       "| 1002 | 0.4740011 | 0.5259989 |\n",
       "| 1003 | 0.4927877 | 0.5072123 |\n",
       "| 1004 | 0.4938562 | 0.5061438 |\n",
       "\n"
      ],
      "text/plain": [
       "     Down      Up       \n",
       "999  0.4901792 0.5098208\n",
       "1000 0.4792185 0.5207815\n",
       "1001 0.4668185 0.5331815\n",
       "1002 0.4740011 0.5259989\n",
       "1003 0.4927877 0.5072123\n",
       "1004 0.4938562 0.5061438"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#posterior\n",
    "lda_pos <- lda_pred$posterior\n",
    "head(lda_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71663182",
   "metadata": {},
   "source": [
    "Applying a 50 % threshold to the posterior probabilities allows us to recreate the predictions contained in *lda_pred$class*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edb1fd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "70"
      ],
      "text/latex": [
       "70"
      ],
      "text/markdown": [
       "70"
      ],
      "text/plain": [
       "[1] 70"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "182"
      ],
      "text/latex": [
       "182"
      ],
      "text/markdown": [
       "182"
      ],
      "text/plain": [
       "[1] 182"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sum(lda_pos[, 1] >= .5)\n",
    "sum(lda_pos[, 1] <= .5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceff93d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.55952380952381"
      ],
      "text/latex": [
       "0.55952380952381"
      ],
      "text/markdown": [
       "0.55952380952381"
      ],
      "text/plain": [
       "[1] 0.5595238"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#accuracy level\n",
    "mean(lda_pred$class == Direction_2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bb46fd",
   "metadata": {},
   "source": [
    "The accuracy of LDA classification is 56%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6d8e2d",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "## QDA theory\n",
    "\n",
    "Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix. That is, it assumes that an observation from the $k$th class is of the form $X ~ N(\\mu_k, \\Sigma_k)$, where $\\Sigma_k$ is a covariance matrix for the $k$th class. Under this assumption, the Bayes classifier assigns an observation $X = x$ to the class for which\n",
    "\n",
    "$\\delta_k(x) = -\\dfrac{1}{2}(x−\\mu_k)^T \\Sigma^{−1}_k (x−\\mu_k)− \\dfrac{1}{2}\\log|\\Sigma_k|+ \\log\\pi_k$\n",
    "\n",
    "is largest. So the QDA classifier involves plugging estimates for $\\Sigma_k, \\mu_k$, and $\\pi_k$ into the above equation, and then assigning an observation $X = x$ to the class for which this quantity is largest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f4e2d0",
   "metadata": {},
   "source": [
    "## QDA applied to 'Smarket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ff7f517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "qda(Direction ~ Lag1 + Lag2, subset = train)\n",
       "\n",
       "Prior probabilities of groups:\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Group means:\n",
       "            Lag1        Lag2\n",
       "Down  0.04279022  0.03389409\n",
       "Up   -0.03954635 -0.03132544"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qda_fit <- qda(Direction ~ Lag1 + Lag2, subset = train)\n",
    "qda_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9523f4b",
   "metadata": {},
   "source": [
    "The output contains the group means. But it does not contain the coefficients of the linear discriminants, because the QDA classifier involves a quadratic, rather than a linear, function of the predictors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82e86f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         Direction_2005\n",
       "qda_class Down  Up\n",
       "     Down   30  20\n",
       "     Up     81 121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "qda_class <- predict(qda_fit, Smarket_2005)$class\n",
    "table(qda_class, Direction_2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03957f8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.599206349206349"
      ],
      "text/latex": [
       "0.599206349206349"
      ],
      "text/markdown": [
       "0.599206349206349"
      ],
      "text/plain": [
       "[1] 0.5992063"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#accuracy level\n",
    "mean(qda_class == Direction_2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4761d5d1",
   "metadata": {},
   "source": [
    "The accuracy is 60%, better than the LDA accuracy of 56% above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1d91ae",
   "metadata": {},
   "source": [
    "# Naive Bayes (NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a41b4f",
   "metadata": {},
   "source": [
    "## NB theory\n",
    "\n",
    "The naive Bayes classifier takes a different tack for estimating $f_1(x),\\ldots, f_K(x)$. Instead of assuming that these functions belong to a particular family of distributions (e.g. multivariate normal), instead the following  single assumption is made:\n",
    "\n",
    "$f_k(x) = \\prod_{j=1}^p f_{k\\,j}(x_j), \\qquad k = 1, \\ldots, K$\n",
    "\n",
    "where $f_{kj}$ is the density function of the $j$th predictor among observations in the $k$th class.\n",
    "\n",
    "This leads to:\n",
    "\n",
    "$p_k(x) = \\dfrac{\\pi_k  \\prod_{j=1}^p f_{kj}(x_j)}{\\sum_{l=1}^K \\pi_l \\prod_{j=1}^p f_{lj}(x_j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93abf499",
   "metadata": {},
   "source": [
    "## NB applied to 'Smarket'\n",
    "\n",
    "Next, we fit a naive Bayes model to the Smarket data. Naive Bayes is implemented in R using the naiveBayes() function, which is part of the *e1071* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c581d205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type 'R' in the Terminal, then 'install.packages('e1071')'.\n",
    "#Choose 16 (China/Beijing2) as the CRAN mirror\n",
    "library(e1071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fbf09e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Naive Bayes Classifier for Discrete Predictors\n",
       "\n",
       "Call:\n",
       "naiveBayes.default(x = X, y = Y, laplace = laplace)\n",
       "\n",
       "A-priori probabilities:\n",
       "Y\n",
       "    Down       Up \n",
       "0.491984 0.508016 \n",
       "\n",
       "Conditional probabilities:\n",
       "      Lag1\n",
       "Y             [,1]     [,2]\n",
       "  Down  0.04279022 1.227446\n",
       "  Up   -0.03954635 1.231668\n",
       "\n",
       "      Lag2\n",
       "Y             [,1]     [,2]\n",
       "  Down  0.03389409 1.239191\n",
       "  Up   -0.03132544 1.220765\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nb_fit <- naiveBayes(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)\n",
    "nb_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7e28726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Direction_2005\n",
       "nb_class Down  Up\n",
       "    Down   28  20\n",
       "    Up     83 121"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#prediction\n",
    "nb_class <- predict(nb_fit, Smarket_2005)\n",
    "table(nb_class, Direction_2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17e7cb21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.591269841269841"
      ],
      "text/latex": [
       "0.591269841269841"
      ],
      "text/markdown": [
       "0.591269841269841"
      ],
      "text/plain": [
       "[1] 0.5912698"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#accuracy\n",
    "mean(nb_class == Direction_2005)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fdf777",
   "metadata": {},
   "source": [
    "Naive Bayes performs very well on this data, with accurate predictions over 59% of the time. This is slightly worse than QDA, but much better than LDA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
